Creator "igraph version 0.10.6 Thu Nov  9 16:12:11 2023"
Version 1
graph
[
  directed 1
  node
  [
    id 0
    NAME "MPI_Comm_dup"
    TIME 0.901277 
    send_rank -2
    recv_rank -2
  ]
  node
  [
    id 1
    NAME "MPI_Bcast"
    TIME 0.013272 
    send_rank 0
    recv_rank 0
  ]
  edge
  [
    source 0
    target 1
  ]
  node
  [
    id 2
    NAME "MPI_Allreduce"
    TIME 114.506095 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 1
    target 2
  ]
  node
  [
    id 3
    NAME "MPI_Alltoall"
    TIME 0.041386 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 2
    target 3
  ]
  node
  [
    id 4
    NAME "MPI_Alltoallv"
    TIME 15.465537 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 3
    target 4
  ]
  node
  [
    id 5
    NAME "MPI_Allreduce"
    TIME 7.175404 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 4
    target 5
  ]
  node
  [
    id 6
    NAME "MPI_Alltoall"
    TIME 0.014435 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 5
    target 6
  ]
  node
  [
    id 7
    NAME "MPI_Alltoallv"
    TIME 6.729307 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 6
    target 7
  ]
  node
  [
    id 8
    NAME "MPI_Allreduce"
    TIME 0.151215 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 7
    target 8
  ]
  node
  [
    id 9
    NAME "MPI_Alltoall"
    TIME 0.009171 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 8
    target 9
  ]
  node
  [
    id 10
    NAME "MPI_Alltoallv"
    TIME 6.362355 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 9
    target 10
  ]
  node
  [
    id 11
    NAME "MPI_Allreduce"
    TIME 0.102485 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 10
    target 11
  ]
  node
  [
    id 12
    NAME "MPI_Alltoall"
    TIME 0.007810 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 11
    target 12
  ]
  node
  [
    id 13
    NAME "MPI_Alltoallv"
    TIME 6.479251 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 12
    target 13
  ]
  node
  [
    id 14
    NAME "MPI_Allreduce"
    TIME 0.073074 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 13
    target 14
  ]
  node
  [
    id 15
    NAME "MPI_Alltoall"
    TIME 0.008725 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 14
    target 15
  ]
  node
  [
    id 16
    NAME "MPI_Alltoallv"
    TIME 6.665669 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 15
    target 16
  ]
  node
  [
    id 17
    NAME "MPI_Allreduce"
    TIME 0.137701 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 16
    target 17
  ]
  node
  [
    id 18
    NAME "MPI_Alltoall"
    TIME 0.008300 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 17
    target 18
  ]
  node
  [
    id 19
    NAME "MPI_Alltoallv"
    TIME 6.512357 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 18
    target 19
  ]
  node
  [
    id 20
    NAME "MPI_Allreduce"
    TIME 0.225383 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 19
    target 20
  ]
  node
  [
    id 21
    NAME "MPI_Alltoall"
    TIME 0.007677 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 20
    target 21
  ]
  node
  [
    id 22
    NAME "MPI_Alltoallv"
    TIME 6.544608 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 21
    target 22
  ]
  node
  [
    id 23
    NAME "MPI_Allreduce"
    TIME 0.368757 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 22
    target 23
  ]
  node
  [
    id 24
    NAME "MPI_Alltoall"
    TIME 0.632455 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 23
    target 24
  ]
  node
  [
    id 25
    NAME "MPI_Alltoallv"
    TIME 8.962990 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 24
    target 25
  ]
  node
  [
    id 26
    NAME "MPI_Allreduce"
    TIME 3.373219 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 25
    target 26
  ]
  node
  [
    id 27
    NAME "MPI_Alltoall"
    TIME 0.008176 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 26
    target 27
  ]
  node
  [
    id 28
    NAME "MPI_Alltoallv"
    TIME 6.236371 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 27
    target 28
  ]
  node
  [
    id 29
    NAME "MPI_Allreduce"
    TIME 4.019085 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 28
    target 29
  ]
  node
  [
    id 30
    NAME "MPI_Alltoall"
    TIME 0.008684 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 29
    target 30
  ]
  node
  [
    id 31
    NAME "MPI_Alltoallv"
    TIME 6.271289 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 30
    target 31
  ]
  node
  [
    id 32
    NAME "MPI_Allreduce"
    TIME 6.558091 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 31
    target 32
  ]
  node
  [
    id 33
    NAME "MPI_Alltoall"
    TIME 0.009008 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 32
    target 33
  ]
  node
  [
    id 34
    NAME "MPI_Alltoallv"
    TIME 6.481004 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 33
    target 34
  ]
  node
  [
    id 35
    NAME "MPI_Reduce"
    TIME 0.594599 
    send_rank 0
    recv_rank 0
  ]
  edge
  [
    source 34
    target 35
  ]
  node
  [
    id 36
    NAME "MPI_Send"
    TIME 0.017382 
    send_rank 0
    recv_rank 1
  ]
  edge
  [
    source 35
    target 36
  ]
  node
  [
    id 37
    NAME "MPI_Reduce"
    TIME 10.943848 
    send_rank 0
    recv_rank 0
  ]
  edge
  [
    source 36
    target 37
  ]
  node
  [
    id 38
    NAME "MPI_Comm_dup"
    TIME 0.233196 
    send_rank -2
    recv_rank -2
  ]
  node
  [
    id 39
    NAME "MPI_Bcast"
    TIME 0.016035 
    send_rank 0
    recv_rank 1
  ]
  edge
  [
    source 38
    target 39
  ]
  node
  [
    id 40
    NAME "MPI_Allreduce"
    TIME 0.109545 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 39
    target 40
  ]
  node
  [
    id 41
    NAME "MPI_Alltoall"
    TIME 0.027452 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 40
    target 41
  ]
  node
  [
    id 42
    NAME "MPI_Alltoallv"
    TIME 15.505053 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 41
    target 42
  ]
  node
  [
    id 43
    NAME "MPI_Allreduce"
    TIME 0.045362 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 42
    target 43
  ]
  node
  [
    id 44
    NAME "MPI_Alltoall"
    TIME 0.013978 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 43
    target 44
  ]
  node
  [
    id 45
    NAME "MPI_Alltoallv"
    TIME 6.729411 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 44
    target 45
  ]
  node
  [
    id 46
    NAME "MPI_Allreduce"
    TIME 0.038021 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 45
    target 46
  ]
  node
  [
    id 47
    NAME "MPI_Alltoall"
    TIME 0.009396 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 46
    target 47
  ]
  node
  [
    id 48
    NAME "MPI_Alltoallv"
    TIME 6.362460 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 47
    target 48
  ]
  node
  [
    id 49
    NAME "MPI_Allreduce"
    TIME 0.034195 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 48
    target 49
  ]
  node
  [
    id 50
    NAME "MPI_Alltoall"
    TIME 0.008517 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 49
    target 50
  ]
  node
  [
    id 51
    NAME "MPI_Alltoallv"
    TIME 6.468292 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 50
    target 51
  ]
  node
  [
    id 52
    NAME "MPI_Allreduce"
    TIME 0.043807 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 51
    target 52
  ]
  node
  [
    id 53
    NAME "MPI_Alltoall"
    TIME 0.008780 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 52
    target 53
  ]
  node
  [
    id 54
    NAME "MPI_Alltoallv"
    TIME 6.659096 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 53
    target 54
  ]
  node
  [
    id 55
    NAME "MPI_Allreduce"
    TIME 0.025530 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 54
    target 55
  ]
  node
  [
    id 56
    NAME "MPI_Alltoall"
    TIME 0.008660 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 55
    target 56
  ]
  node
  [
    id 57
    NAME "MPI_Alltoallv"
    TIME 6.505775 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 56
    target 57
  ]
  node
  [
    id 58
    NAME "MPI_Allreduce"
    TIME 0.029843 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 57
    target 58
  ]
  node
  [
    id 59
    NAME "MPI_Alltoall"
    TIME 0.007660 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 58
    target 59
  ]
  node
  [
    id 60
    NAME "MPI_Alltoallv"
    TIME 6.544253 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 59
    target 60
  ]
  node
  [
    id 61
    NAME "MPI_Allreduce"
    TIME 0.089512 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 60
    target 61
  ]
  node
  [
    id 62
    NAME "MPI_Alltoall"
    TIME 0.634073 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 61
    target 62
  ]
  node
  [
    id 63
    NAME "MPI_Alltoallv"
    TIME 8.880492 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 62
    target 63
  ]
  node
  [
    id 64
    NAME "MPI_Allreduce"
    TIME 3.125309 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 63
    target 64
  ]
  node
  [
    id 65
    NAME "MPI_Alltoall"
    TIME 0.007659 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 64
    target 65
  ]
  node
  [
    id 66
    NAME "MPI_Alltoallv"
    TIME 6.249284 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 65
    target 66
  ]
  node
  [
    id 67
    NAME "MPI_Allreduce"
    TIME 0.884208 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 66
    target 67
  ]
  node
  [
    id 68
    NAME "MPI_Alltoall"
    TIME 0.008137 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 67
    target 68
  ]
  node
  [
    id 69
    NAME "MPI_Alltoallv"
    TIME 6.360923 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 68
    target 69
  ]
  node
  [
    id 70
    NAME "MPI_Allreduce"
    TIME 0.881494 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 69
    target 70
  ]
  node
  [
    id 71
    NAME "MPI_Alltoall"
    TIME 0.008183 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 70
    target 71
  ]
  node
  [
    id 72
    NAME "MPI_Alltoallv"
    TIME 6.569627 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 71
    target 72
  ]
  node
  [
    id 73
    NAME "MPI_Reduce"
    TIME 0.029866 
    send_rank 1
    recv_rank 0
  ]
  edge
  [
    source 72
    target 73
  ]
  node
  [
    id 74
    NAME "MPI_Irecv"
    TIME 0.005619 
    send_rank 0
    recv_rank 1
  ]
  edge
  [
    source 73
    target 74
  ]
  node
  [
    id 75
    NAME "MPI_Send"
    TIME 0.016973 
    send_rank 1
    recv_rank 2
  ]
  edge
  [
    source 74
    target 75
  ]
  node
  [
    id 76
    NAME "MPI_Wait"
    TIME 0.012478 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 75
    target 76
  ]
  node
  [
    id 77
    NAME "MPI_Reduce"
    TIME 0.010077 
    send_rank 1
    recv_rank 0
  ]
  edge
  [
    source 76
    target 77
  ]
  node
  [
    id 78
    NAME "MPI_Comm_dup"
    TIME 0.209438 
    send_rank -2
    recv_rank -2
  ]
  node
  [
    id 79
    NAME "MPI_Bcast"
    TIME 0.016012 
    send_rank 0
    recv_rank 2
  ]
  edge
  [
    source 78
    target 79
  ]
  node
  [
    id 80
    NAME "MPI_Allreduce"
    TIME 336.439683 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 79
    target 80
  ]
  node
  [
    id 81
    NAME "MPI_Alltoall"
    TIME 0.050152 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 80
    target 81
  ]
  node
  [
    id 82
    NAME "MPI_Alltoallv"
    TIME 15.505671 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 81
    target 82
  ]
  node
  [
    id 83
    NAME "MPI_Allreduce"
    TIME 12.241881 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 82
    target 83
  ]
  node
  [
    id 84
    NAME "MPI_Alltoall"
    TIME 0.016049 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 83
    target 84
  ]
  node
  [
    id 85
    NAME "MPI_Alltoallv"
    TIME 6.702019 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 84
    target 85
  ]
  node
  [
    id 86
    NAME "MPI_Allreduce"
    TIME 6.224970 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 85
    target 86
  ]
  node
  [
    id 87
    NAME "MPI_Alltoall"
    TIME 0.010228 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 86
    target 87
  ]
  node
  [
    id 88
    NAME "MPI_Alltoallv"
    TIME 6.333630 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 87
    target 88
  ]
  node
  [
    id 89
    NAME "MPI_Allreduce"
    TIME 6.402774 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 88
    target 89
  ]
  node
  [
    id 90
    NAME "MPI_Alltoall"
    TIME 0.009530 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 89
    target 90
  ]
  node
  [
    id 91
    NAME "MPI_Alltoallv"
    TIME 6.478951 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 90
    target 91
  ]
  node
  [
    id 92
    NAME "MPI_Allreduce"
    TIME 6.215748 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 91
    target 92
  ]
  node
  [
    id 93
    NAME "MPI_Alltoall"
    TIME 0.009335 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 92
    target 93
  ]
  node
  [
    id 94
    NAME "MPI_Alltoallv"
    TIME 6.665433 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 93
    target 94
  ]
  node
  [
    id 95
    NAME "MPI_Allreduce"
    TIME 6.148615 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 94
    target 95
  ]
  node
  [
    id 96
    NAME "MPI_Alltoall"
    TIME 0.008311 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 95
    target 96
  ]
  node
  [
    id 97
    NAME "MPI_Alltoallv"
    TIME 6.509222 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 96
    target 97
  ]
  node
  [
    id 98
    NAME "MPI_Allreduce"
    TIME 6.288419 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 97
    target 98
  ]
  node
  [
    id 99
    NAME "MPI_Alltoall"
    TIME 0.009181 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 98
    target 99
  ]
  node
  [
    id 100
    NAME "MPI_Alltoallv"
    TIME 6.531714 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 99
    target 100
  ]
  node
  [
    id 101
    NAME "MPI_Allreduce"
    TIME 6.784213 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 100
    target 101
  ]
  node
  [
    id 102
    NAME "MPI_Alltoall"
    TIME 0.046949 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 101
    target 102
  ]
  node
  [
    id 103
    NAME "MPI_Alltoallv"
    TIME 9.024727 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 102
    target 103
  ]
  node
  [
    id 104
    NAME "MPI_Allreduce"
    TIME 0.041564 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 103
    target 104
  ]
  node
  [
    id 105
    NAME "MPI_Alltoall"
    TIME 0.008338 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 104
    target 105
  ]
  node
  [
    id 106
    NAME "MPI_Alltoallv"
    TIME 6.222590 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 105
    target 106
  ]
  node
  [
    id 107
    NAME "MPI_Allreduce"
    TIME 0.038194 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 106
    target 107
  ]
  node
  [
    id 108
    NAME "MPI_Alltoall"
    TIME 0.007837 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 107
    target 108
  ]
  node
  [
    id 109
    NAME "MPI_Alltoallv"
    TIME 6.361846 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 108
    target 109
  ]
  node
  [
    id 110
    NAME "MPI_Allreduce"
    TIME 0.056752 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 109
    target 110
  ]
  node
  [
    id 111
    NAME "MPI_Alltoall"
    TIME 0.008584 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 110
    target 111
  ]
  node
  [
    id 112
    NAME "MPI_Alltoallv"
    TIME 6.569700 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 111
    target 112
  ]
  node
  [
    id 113
    NAME "MPI_Reduce"
    TIME 0.701209 
    send_rank 2
    recv_rank 0
  ]
  edge
  [
    source 112
    target 113
  ]
  node
  [
    id 114
    NAME "MPI_Irecv"
    TIME 0.008614 
    send_rank 1
    recv_rank 2
  ]
  edge
  [
    source 113
    target 114
  ]
  node
  [
    id 115
    NAME "MPI_Send"
    TIME 0.015671 
    send_rank 2
    recv_rank 3
  ]
  edge
  [
    source 114
    target 115
  ]
  node
  [
    id 116
    NAME "MPI_Wait"
    TIME 16.121736 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 115
    target 116
  ]
  node
  [
    id 117
    NAME "MPI_Reduce"
    TIME 0.013966 
    send_rank 2
    recv_rank 0
  ]
  edge
  [
    source 116
    target 117
  ]
  node
  [
    id 118
    NAME "MPI_Comm_dup"
    TIME 0.344744 
    send_rank -2
    recv_rank -2
  ]
  node
  [
    id 119
    NAME "MPI_Bcast"
    TIME 0.017557 
    send_rank 0
    recv_rank 3
  ]
  edge
  [
    source 118
    target 119
  ]
  node
  [
    id 120
    NAME "MPI_Allreduce"
    TIME 114.809340 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 119
    target 120
  ]
  node
  [
    id 121
    NAME "MPI_Alltoall"
    TIME 0.049035 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 120
    target 121
  ]
  node
  [
    id 122
    NAME "MPI_Alltoallv"
    TIME 15.483875 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 121
    target 122
  ]
  node
  [
    id 123
    NAME "MPI_Allreduce"
    TIME 7.761115 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 122
    target 123
  ]
  node
  [
    id 124
    NAME "MPI_Alltoall"
    TIME 0.014836 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 123
    target 124
  ]
  node
  [
    id 125
    NAME "MPI_Alltoallv"
    TIME 6.714905 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 124
    target 125
  ]
  node
  [
    id 126
    NAME "MPI_Allreduce"
    TIME 0.851147 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 125
    target 126
  ]
  node
  [
    id 127
    NAME "MPI_Alltoall"
    TIME 0.009335 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 126
    target 127
  ]
  node
  [
    id 128
    NAME "MPI_Alltoallv"
    TIME 6.349519 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 127
    target 128
  ]
  node
  [
    id 129
    NAME "MPI_Allreduce"
    TIME 0.971556 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 128
    target 129
  ]
  node
  [
    id 130
    NAME "MPI_Alltoall"
    TIME 0.008854 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 129
    target 130
  ]
  node
  [
    id 131
    NAME "MPI_Alltoallv"
    TIME 6.474899 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 130
    target 131
  ]
  node
  [
    id 132
    NAME "MPI_Allreduce"
    TIME 0.853308 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 131
    target 132
  ]
  node
  [
    id 133
    NAME "MPI_Alltoall"
    TIME 0.008853 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 132
    target 133
  ]
  node
  [
    id 134
    NAME "MPI_Alltoallv"
    TIME 6.662109 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 133
    target 134
  ]
  node
  [
    id 135
    NAME "MPI_Allreduce"
    TIME 0.853203 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 134
    target 135
  ]
  node
  [
    id 136
    NAME "MPI_Alltoall"
    TIME 0.007942 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 135
    target 136
  ]
  node
  [
    id 137
    NAME "MPI_Alltoallv"
    TIME 6.511511 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 136
    target 137
  ]
  node
  [
    id 138
    NAME "MPI_Allreduce"
    TIME 0.935070 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 137
    target 138
  ]
  node
  [
    id 139
    NAME "MPI_Alltoall"
    TIME 0.007958 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 138
    target 139
  ]
  node
  [
    id 140
    NAME "MPI_Alltoallv"
    TIME 6.440788 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 139
    target 140
  ]
  node
  [
    id 141
    NAME "MPI_Allreduce"
    TIME 1.001101 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 140
    target 141
  ]
  node
  [
    id 142
    NAME "MPI_Alltoall"
    TIME 0.628741 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 141
    target 142
  ]
  node
  [
    id 143
    NAME "MPI_Alltoallv"
    TIME 9.028756 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 142
    target 143
  ]
  node
  [
    id 144
    NAME "MPI_Allreduce"
    TIME 3.755098 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 143
    target 144
  ]
  node
  [
    id 145
    NAME "MPI_Alltoall"
    TIME 0.007147 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 144
    target 145
  ]
  node
  [
    id 146
    NAME "MPI_Alltoallv"
    TIME 6.248545 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 145
    target 146
  ]
  node
  [
    id 147
    NAME "MPI_Allreduce"
    TIME 1.831561 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 146
    target 147
  ]
  node
  [
    id 148
    NAME "MPI_Alltoall"
    TIME 0.007266 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 147
    target 148
  ]
  node
  [
    id 149
    NAME "MPI_Alltoallv"
    TIME 6.250913 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 148
    target 149
  ]
  node
  [
    id 150
    NAME "MPI_Allreduce"
    TIME 5.230461 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 149
    target 150
  ]
  node
  [
    id 151
    NAME "MPI_Alltoall"
    TIME 0.007413 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 150
    target 151
  ]
  node
  [
    id 152
    NAME "MPI_Alltoallv"
    TIME 6.388989 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 151
    target 152
  ]
  node
  [
    id 153
    NAME "MPI_Reduce"
    TIME 0.031608 
    send_rank 3
    recv_rank 0
  ]
  edge
  [
    source 152
    target 153
  ]
  node
  [
    id 154
    NAME "MPI_Irecv"
    TIME 0.005562 
    send_rank 2
    recv_rank 3
  ]
  edge
  [
    source 153
    target 154
  ]
  node
  [
    id 155
    NAME "MPI_Wait"
    TIME 0.014655 
    send_rank -2
    recv_rank -2
  ]
  edge
  [
    source 154
    target 155
  ]
  node
  [
    id 156
    NAME "MPI_Reduce"
    TIME 0.009626 
    send_rank 3
    recv_rank 0
  ]
  edge
  [
    source 155
    target 156
  ]
]
